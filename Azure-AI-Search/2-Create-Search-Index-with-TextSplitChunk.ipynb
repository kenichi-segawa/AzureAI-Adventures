{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import require libraries and envrioment variables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import io\n",
        "import json\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "import time\n",
        "import http.client, urllib.parse\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes import SearchIndexerClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SemanticPrioritizedFields,\n",
        "    SearchableField,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SearchIndex,\n",
        "    SearchIndexerDataContainer,\n",
        "    SearchIndexerDataSourceConnection,\n",
        "    SemanticConfiguration,\n",
        "    SemanticField,\n",
        "    SimpleField,\n",
        "    VectorSearch,\n",
        "    VectorSearchAlgorithmConfiguration,\n",
        "    IndexProjectionMode,\n",
        "    SearchIndexer,\n",
        "    FieldMapping,\n",
        "    SplitSkill\n",
        ")\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  \n",
        "\n",
        "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
        "api_version = os.getenv(\"AZURE_SEARCH_API_VERSION\")\n",
        "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
        "credential = AzureKeyCredential(key)\n",
        "\n",
        "blob_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
        "blob_base_url = os.getenv(\"AZURE_STORAGE_BASE_URL\")\n",
        "\n",
        "# Setup the Payloads header for cog search\n",
        "headers = {'Content-Type': 'application/json','api-key': key}\n",
        "params = {'api-version': api_version}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740328338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_status(index_name):\n",
        "    print(\"Azure Cognitive Search Index:\", index_name, \"\\n\")\n",
        "\n",
        "    index_status = requests.get(\n",
        "        service_endpoint + \"/indexes/\" + index_name, headers=headers, params=params\n",
        "    )\n",
        "    try:\n",
        "        print(json.dumps((index_status.json()), indent=5))\n",
        "    except:\n",
        "        print(\"Request failed\")\n",
        "\n",
        "def index_stats(index_name):\n",
        "    url = (\n",
        "        service_endpoint\n",
        "        + \"/indexes/\"\n",
        "        + index_name\n",
        "        + \"/stats?api-version=\"\n",
        "        + api_version\n",
        "    )\n",
        "    \n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(\"Azure AI Search index status for:\", index_name, \"\\n\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        res = response.json()\n",
        "        print(json.dumps(res, indent=2))\n",
        "        document_count = res[\"documentCount\"]\n",
        "        storage_size = res[\"storageSize\"]\n",
        "\n",
        "    else:\n",
        "        print(\"Request failed with status code:\", response.status_code)\n",
        "\n",
        "    return document_count, storage_size\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740329868
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Blob Storage\n",
        "\n",
        "Retrieve documents from blob storage. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get files from data folder\n",
        "data_folder = \"data\"\n",
        "filename = \"rao-barista.pdf\"\n",
        "\n",
        "container_name = 'blob-rao-barista-file'\n",
        "\n",
        "# connect to blob storage\n",
        "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "if not container_client.exists():\n",
        "    container_client.create_container()\n",
        "\n",
        "# upload data to blob storage\n",
        "documents_directory = os.path.join(\"data\")\n",
        "for file in os.listdir(documents_directory): \n",
        "    if file == filename:\n",
        "        with open(os.path.join(documents_directory, file), \"rb\") as data:\n",
        "            name = os.path.basename(file)\n",
        "            if not container_client.get_blob_client(name).exists():\n",
        "                print(f'Uploading {name} to blob storage...')\n",
        "                container_client.upload_blob(name=name, data=data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740332415
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect your Blob storage to a data source in Azure AI search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'ksegawa-index-w-textsplitchunking'\n",
        "\n",
        "# create a data source\n",
        "# connect to the search service indexer\n",
        "indexer_client = SearchIndexerClient(service_endpoint, credential)\n",
        "\n",
        "# create a data source\n",
        "container = SearchIndexerDataContainer(name=container_name)\n",
        "\n",
        "# create a data source connection\n",
        "data_source_connection = SearchIndexerDataSourceConnection(\n",
        "    name=f\"{index_name}-connection\",\n",
        "    type=\"azureblob\",\n",
        "    connection_string=blob_connection_string,\n",
        "    container=container\n",
        ")\n",
        "\n",
        "# create or update the data source\n",
        "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
        "\n",
        "# print the data source\n",
        "print(f\"Data source '{data_source.name}' created or updated\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740333943
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a search index\n",
        "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
        "\n",
        "# define the fields for the index\n",
        "fields = [  \n",
        "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
        "    SearchField(name=\"title\", type=SearchFieldDataType.String),  \n",
        "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
        "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
        "]  \n",
        "\n",
        "scoring_profiles = []\n",
        "\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    scoring_profiles=scoring_profiles)\n",
        "\n",
        "result = index_client.create_or_update_index(index)\n",
        "result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740335550
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an indexer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Required to use the preview SDK\n",
        "from azure.search.documents.indexes._generated.models import (\n",
        "    SearchIndexerSkillset,\n",
        "    AzureOpenAIVectorizer,\n",
        "    AzureOpenAIParameters,\n",
        "    SearchIndexerIndexProjections,\n",
        "    SearchIndexerIndexProjectionSelector,\n",
        "    SearchIndexerIndexProjectionsParameters,\n",
        "    InputFieldMappingEntry,\n",
        "    OutputFieldMappingEntry\n",
        ")\n",
        "\n",
        "# Create an indexer\n",
        "indexer_name = f\"{index_name}-indexer\"\n",
        "skillset_name = f\"{index_name}-chunk-skillset\"\n",
        "\n",
        "indexer_client = SearchIndexerClient(service_endpoint, credential)\n",
        "\n",
        "index_projections = SearchIndexerIndexProjections(  \n",
        "    selectors=[  \n",
        "        SearchIndexerIndexProjectionSelector(  \n",
        "            target_index_name=index_name,  \n",
        "            parent_key_field_name=\"parent_id\",  \n",
        "            source_context=\"/document/pages/*\",  \n",
        "            mappings=[  \n",
        "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
        "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
        "            ],  \n",
        "        ),  \n",
        "    ],  \n",
        "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
        "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
        "    ),  \n",
        ")  \n",
        "\n",
        "skillset = SearchIndexerSkillset(\n",
        "        name=skillset_name,\n",
        "        skills=[\n",
        "            SplitSkill(\n",
        "                name=\"Text Splitter\",\n",
        "                default_language_code=\"en\",\n",
        "                text_split_mode='pages',\n",
        "                maximum_page_length=500,\n",
        "                page_overlap_length=0,\n",
        "                context=\"/document\",\n",
        "                inputs=[\n",
        "                    InputFieldMappingEntry(\n",
        "                        name=\"text\",\n",
        "                        source=\"/document/content\"\n",
        "                    )\n",
        "                ],\n",
        "                outputs=[\n",
        "                    OutputFieldMappingEntry(\n",
        "                        name=\"textItems\",\n",
        "                        target_name=\"pages\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ],\n",
        "        index_projections=index_projections\n",
        ")\n",
        "            \n",
        "indexer_client.create_or_update_skillset(skillset)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740338410
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer\n",
        "try:           \n",
        "    indexer = SearchIndexer(  \n",
        "        name=indexer_name,  \n",
        "        description=\"Indexer to index documents\",  \n",
        "        target_index_name=index_name,  \n",
        "        data_source_name=data_source.name,  \n",
        "        skillset_name=skillset_name\n",
        "    )\n",
        "\n",
        "    indexer_result = indexer_client.create_or_update_indexer(indexer)\n",
        "\n",
        "    # Run the indexer  \n",
        "    indexer_client.run_indexer(indexer_name) \n",
        "    print(f' {indexer_name} is created and running. If queries return no results, please wait a bit and try again.')\n",
        "except Exception as ex:\n",
        "    # if the indexer fails, delete and retry again\n",
        "    print(ex)\n",
        "    indexer_client.delete_indexer(indexer_name)\n",
        "    indexer_client.create_or_update_indexer(indexer)\n",
        "    indexer_client.run_indexer(indexer_name)\n",
        "    print(f' {indexer_name} is re-created and running. If queries return no results, please wait a bit and try again.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740358474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_stats(index_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740359059
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"grinder coffee\"\n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "results = search_client.search(  \n",
        "    search_text=query,  \n",
        "    select=[\"chunk_id\", \"chunk\", \"title\"],\n",
        "    top=1\n",
        ")  \n",
        "\n",
        "for result in results: \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"title: {result['title']}\") \n",
        "    print(f\"content: {result['chunk']}\")  \n",
        "    print('===================================================')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713740359298
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the key steps a barista should follow to prepare an espresso with uniform resistance to water?\"\n",
        "\n",
        "results = search_client.search(  \n",
        "    search_text=query,  \n",
        "    select=[\"parent_id\", \"chunk_id\", \"chunk\", \"title\"],\n",
        "    top=1\n",
        ")  \n",
        "\n",
        "for result in results:  \n",
        "    print(f\"Score: {result['@search.score']}\")  \n",
        "    print(f\"parent_id: {result['parent_id']}\")  \n",
        "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
        "    print(f\"title: {result['title']}\") \n",
        "    print(f\"Chunk: {result['chunk']}\") \n",
        "    print('===================================================')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# delete index, indexer, skillsest\n",
        "\n",
        "index_client.delete_index(index_name)\n",
        "index_client.delete_indexer(indexer_name)\n",
        "index_client.delete_skillset(skillset_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}