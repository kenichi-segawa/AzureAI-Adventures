{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain data chunking example\n",
        "\n",
        "This notebook uses Langchain's recursive character text splitter to chunk text. Source files are large PDFs loaded using PyPDFLoader.\n",
        "\n",
        "The notebook complements the [Chunking large documents for vector search solutions](https://learn.microsoft.com/azure/search/vector-search-how-to-chunk-document) article in the Azure AI Search documentation.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install packages"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load .env file (Copy .env-sample to .env and update accordingly)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "import os\n",
        "\n",
        "load_dotenv() # take environment variables from .env\n",
        "\n",
        "# variables not used here do not need to be updated in your .env file\n",
        "search_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
        "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        "azure_openai_embedding_deployment_id = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL\"]\n",
        "recursivetextsplitter_searchindex = 'chunkingsample-recursivetextsplitter_langchain'\n",
        "\n",
        "search_credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
        "azure_openai_key = os.environ[\"AZURE_OPENAI_API_KEY\"] if len(os.environ[\"AZURE_OPENAI_API_KEY\"]) > 0 else None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741412464
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup sample resources for embedding chunks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "# from azure.identity import get_bearer_token_provider\n",
        "\n",
        "azure_openai_client = None\n",
        "if azure_openai_key:\n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        api_key=azure_openai_key, \n",
        "        api_version=\"2023-05-15\",\n",
        "        azure_deployment=azure_openai_embedding_deployment_id,\n",
        "        azure_endpoint=azure_openai_endpoint)\n",
        "else:\n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
        "        api_version=\"2023-05-15\",\n",
        "        azure_deployment=azure_openai_embedding_deployment_id,\n",
        "        azure_endpoint=azure_openai_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741413518
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup sample resources for recursive text splitter chunking"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "! pip uninstall -y azure-search-documents \n",
        "! pip install azure-search-documents==11.6.0b2"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex,\n",
        "    SearchIndexer,\n",
        "    SearchIndexerDataSourceConnection,\n",
        "    SearchIndexerDataContainer,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    VectorSearch,\n",
        "    HnswAlgorithmConfiguration,\n",
        "    HnswParameters,\n",
        "    VectorSearchAlgorithmMetric,\n",
        "    AzureOpenAIEmbeddingSkill,\n",
        "    SplitSkill,\n",
        "    VectorSearchProfile\n",
        ")\n",
        "# Required to use the preview SDK\n",
        "from azure.search.documents.indexes._generated.models import (\n",
        "    SearchIndexerSkillset,\n",
        "    AzureOpenAIVectorizer,\n",
        "    AzureOpenAIParameters,\n",
        "    SearchIndexerIndexProjections,\n",
        "    SearchIndexerIndexProjectionSelector,\n",
        "    SearchIndexerIndexProjectionsParameters,\n",
        "    InputFieldMappingEntry,\n",
        "    OutputFieldMappingEntry\n",
        ")\n",
        "\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741415073
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_field = [\n",
        "        SearchField(\n",
        "            name=\"chunk_id\",\n",
        "            type=SearchFieldDataType.String,\n",
        "            key=True,\n",
        "            hidden=False,\n",
        "            filterable=True,\n",
        "            sortable=True,\n",
        "            facetable=False,\n",
        "            searchable=True,\n",
        "            analyzer_name=\"keyword\"\n",
        "        ),\n",
        "        SearchField(\n",
        "            name=\"parent_id\",\n",
        "            type=SearchFieldDataType.String,\n",
        "            hidden=False,\n",
        "            filterable=True,\n",
        "            sortable=True,\n",
        "            facetable=False,\n",
        "            searchable=True\n",
        "        ),\n",
        "        SearchField(\n",
        "            name=\"chunk\",\n",
        "            type=SearchFieldDataType.String,\n",
        "            hidden=False,\n",
        "            filterable=False,\n",
        "            sortable=False,\n",
        "            facetable=False,\n",
        "            searchable=True\n",
        "        ),\n",
        "        SearchField(\n",
        "            name=\"title\",\n",
        "            type=SearchFieldDataType.String,\n",
        "            hidden=False,\n",
        "            filterable=False,\n",
        "            sortable=False,\n",
        "            facetable=False,\n",
        "            searchable=True\n",
        "        ),\n",
        "        SearchField(\n",
        "            name=\"vector\",\n",
        "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "            hidden=False,\n",
        "            filterable=False,\n",
        "            sortable=False,\n",
        "            facetable=False,\n",
        "            searchable=True,\n",
        "            vector_search_dimensions=1536,\n",
        "            vector_search_profile=\"profile\"\n",
        "        )\n",
        "        ]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741416138
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search = VectorSearch(\n",
        "            profiles=[\n",
        "                VectorSearchProfile(\n",
        "                    name=\"profile\",\n",
        "                    algorithm_configuration_name=\"hnsw-algorithm\",\n",
        "                    vectorizer=\"azure-openai-vectorizer\"\n",
        "                )\n",
        "            ],\n",
        "            algorithms=[\n",
        "                HnswAlgorithmConfiguration(  \n",
        "            name=\"myHnsw\",  \n",
        "            parameters=HnswParameters(  \n",
        "                m=4,  \n",
        "                ef_construction=400,  \n",
        "                ef_search=500,  \n",
        "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
        "            ),  \n",
        "        )\n",
        "            ],\n",
        "            vectorizers=[\n",
        "                AzureOpenAIVectorizer(\n",
        "                        name=\"azure-openai-vectorizer\",\n",
        "                        azure_open_ai_parameters=AzureOpenAIParameters(\n",
        "                            resource_uri=azure_openai_endpoint,\n",
        "                            deployment_id=azure_openai_embedding_deployment_id,\n",
        "                            api_key=azure_openai_key # Optional if using RBAC authentication\n",
        "                        )\n",
        "                    )\n",
        "            ]\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741416946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "\n",
        "search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
        "rts_searchindex = SearchIndex(\n",
        "        name=recursivetextsplitter_searchindex,\n",
        "        fields=search_field,\n",
        "        vector_search=vector_search)\n",
        "\n",
        "search_index_client.create_or_update_index(rts_searchindex)\n",
        "\n",
        "print(\"Created recursive text splitter index\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741418804
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load PDF"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "loader = PyPDFLoader(os.path.join(\"data\", \"roa-barista.pdf\"))\n",
        "pages = loader.load()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741418898
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1713741419447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate histogram of token and character lengths per page"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_chunk_histogram(chunks, length_fn, title, xlabel, ylabel=\"Chunk Count\"):\n",
        "    def round_to_lowest_multiple(number, multiple):\n",
        "        return (number // multiple) * multiple\n",
        "\n",
        "    def round_to_highest_multiple(number, multiple):\n",
        "        return math.ceil(number / multiple) * multiple\n",
        "\n",
        "    ys = [length_fn(chunk) for chunk in chunks]\n",
        "    min_y = min(ys)\n",
        "    max_y = max(ys)\n",
        "    bins=25\n",
        "    n, _, _ = plt.hist(ys, edgecolor=\"black\", bins=bins) \n",
        "    # Set y-axis limits to remove the gap at the top\n",
        "    max_freq = max(n)\n",
        "    plt.ylim(0, max_freq)\n",
        "\n",
        "    # Spacing for ticks on x-axis and x-axis limits to remove gaps\n",
        "    tick_step = max(int(round_to_lowest_multiple((max_y-min_y)/5, 100)), 100)\n",
        "    max_xtick = round_to_highest_multiple(max_y, tick_step)\n",
        "    xticks = list(np.arange(start=round_to_lowest_multiple(min_y, tick_step), stop=round_to_highest_multiple(max_xtick, tick_step), step=tick_step))\n",
        "    if max_xtick and xticks[-1] != max_xtick:\n",
        "        xticks.append(max_xtick)\n",
        "    plt.xticks(xticks)\n",
        "    plt.xlim(round_to_lowest_multiple(min_y, tick_step), max_xtick)\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    \n",
        "def get_token_length(text, model=\"gpt-3.5-turbo\"):\n",
        "    return len(tiktoken.encoding_for_model(model).encode(text))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "page_content = [page.page_content for page in pages]\n",
        "\n",
        "plot_chunk_histogram(\n",
        "    chunks=page_content,\n",
        "    length_fn=len,\n",
        "    title=\"Distribution of page character lengths\",\n",
        "    xlabel=\"Page character length\",\n",
        "    ylabel=\"Page count\")\n",
        "\n",
        "plot_chunk_histogram(\n",
        "    chunks=page_content,\n",
        "    length_fn=get_token_length,\n",
        "    title=\"Distribution of page token lengths\",\n",
        "    xlabel=\"Page token length\",\n",
        "    ylabel=\"Page count\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunk PDF using Recursive text splitter\n",
        "\n",
        "We use the output of the above historgrams to guide us into selecting a 600 token chunk length with a 150 token overlap."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoding_name(model=\"gpt-3.5-turbo\"):\n",
        "    return tiktoken.encoding_for_model(model).name"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# from_tiktoken_encoder enables use to split on tokens rather than characters\n",
        "recursive_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "   encoding_name=get_encoding_name(),\n",
        "   chunk_size=600, \n",
        "   chunk_overlap=125\n",
        ")\n",
        "\n",
        "recursive_text_splitter_chunks = recursive_text_splitter.split_documents(pages)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for x, y in zip([pages for pages in page_content], [page_r.page_content for page_r in recursive_text_splitter_chunks]):\n",
        "#     print(f\"Original: {x} |||||||||||||||| Recursive Text Splitter: {y}\")\n",
        "#     print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate histogram of chunk character and token lengths"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_content = [chunk.page_content for chunk in recursive_text_splitter_chunks]\n",
        "\n",
        "plot_chunk_histogram(\n",
        "    chunks=chunk_content,\n",
        "    length_fn=len,\n",
        "    title=\"Distribution of chunk character lengths\",\n",
        "    xlabel=\"Chunk character length\")\n",
        "plot_chunk_histogram(\n",
        "    chunks=chunk_content,\n",
        "    length_fn=get_token_length,\n",
        "    title=\"Distribution of chunk token lengths\",\n",
        "    xlabel=\"Chunk token length\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed Recursive text splitter chunks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_text_splitter_embeddings = azure_openai_client.embeddings.create(input=chunk_content, model=azure_openai_embedding_deployment_id)\n",
        "recursive_text_splitter_embeddings = [result.embedding for result in recursive_text_splitter_embeddings.data]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload chunks to search index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_search_client = search_index_client.get_search_client(recursivetextsplitter_searchindex)\n",
        "\n",
        "docs = [\n",
        "    {\n",
        "        \"parent_id\": \"0\",\n",
        "        \"chunk_id\": f\"earth-at-night-508-pdf_0_0_{i}\",\n",
        "        \"chunk\": chunk.page_content,\n",
        "        \"title\": \"earth_at_night_508.pdf\",\n",
        "        \"vector\": recursive_text_splitter_embeddings[i]\n",
        "    }\n",
        "    for i, chunk in enumerate(recursive_text_splitter_chunks)\n",
        "]\n",
        "\n",
        "recursive_search_client.upload_documents(docs)\n",
        "\n",
        "print(\"Uploaded chunks and embeddings for recursive text splitter\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}